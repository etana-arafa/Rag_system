# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-8j1dTdzzkR8vberSJHMkQ6YeGsmRGu4
"""

!pip install google-generativeai langchain faiss-cpu sentence-transformers PyMuPDF

import os
import google.generativeai as genai

# üîë Replace with your own Gemini API key
os.environ["GOOGLE_API_KEY"] = "AIzaSyBqe5cg9NNtHdmVgOoXdoaH5ICwQWjP5zg"

# Configure Gemini
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("models/gemini-1.5-pro")

import fitz  # PyMuPDF
import re

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text()
    return full_text

def clean_text(text):
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'[^\S\r\n]+', ' ', text)
    return text.strip()

raw_text = extract_text_from_pdf("/content/HSC26-Bangla1st-Paper.pdf")
cleaned_text = clean_text(raw_text)

from langchain.text_splitter import RecursiveCharacterTextSplitter

def chunk_text(text):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    return splitter.split_text(text)

chunks = chunk_text(cleaned_text)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = embed_model.encode(chunks, convert_to_numpy=True)

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

def retrieve_top_k(query, k=3):
    query_vec = embed_model.encode([query])
    D, I = index.search(np.array(query_vec), k)
    return [chunks[i] for i in I[0]]

def generate_answer_with_gemini(context_chunks, question):
    context = "\n\n".join(context_chunks)
    prompt = f"""
Use the following context to answer the question. If the answer is not found, say "I don't know".

Context:
{context}

Question:
{question}

Answer:
"""
    response = model.generate_content(prompt)
    return response.text.strip()

questions = [
   "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?"
]

for q in questions:
    top_chunks = retrieve_top_k(q)
    answer = generate_answer_with_gemini(top_chunks, q)
    print(f"‚ùì {q}\n‚úÖ {answer}\n")

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class Query(BaseModel):
    question: str

@app.post("/ask")
def ask_question(data: Query):
    top_chunks = retrieve_top_k(data.question)
    answer = generate_answer(top_chunks, data.question)
    return {"answer": answer}

from sklearn.metrics.pairwise import cosine_similarity

def evaluate_similarity(query, retrieved_chunk):
    q_emb = embedder.encode([query])
    c_emb = embedder.encode([retrieved_chunk])
    score = cosine_similarity(q_emb, c_emb)[0][0]
    return score